\relax 
\citation{Rosenblatt}
\citation{Minsky}
\citation{ann}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Introduction}{1}}
\citation{allancnn}
\citation{allancnn}
\@writefile{toc}{\contentsline {section}{\numberline {1}3D image data}{3}}
\citation{kinect}
\citation{kinect}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces example of the 3D data from the outdoor urban LIDAR scans\cite  {allancnn}.}}{4}}
\newlabel{fig:lidar}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The depth map geenerated by Kinect. The depth map is visualized here using color gradients from white (near) to blue (far)\cite  {kinect}}}{4}}
\newlabel{fig:kinect}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The 3D view from the robot of Fetch's perspective.}}{5}}
\newlabel{fig:fetch}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Image Classification and Object Detection}{5}}
\citation{csaji2001}
\citation{csaji2001}
\citation{hornik1991}
\citation{SimonyanZ14a}
\citation{DBLP:journals/corr/HeZRS15}
\citation{Rumelhart1986}
\citation{NIPS2012_4824}
\citation{SimonyanZ14a}
\citation{DBLP:journals/corr/HeZRS15}
\citation{doi:10.1162/neco.1989.1.4.541}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Deep Learning Theory}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Approximation with Artificial Neural Networks }{6}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Algorithms developed based on 2D Images}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Image Classification}{7}}
\citation{imagenet_cvpr09}
\citation{NIPS2012_4824}
\citation{DBLP:journals/corr/abs-1207-0580}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The Neural Network used in \cite  {doi:10.1162/neco.1989.1.4.541}.}}{8}}
\newlabel{fig:bpzip}{{4}{8}}
\citation{NIPS2012_4824}
\citation{NIPS2012_4824}
\citation{NIPS2012_4824}
\citation{NIPS2012_4824}
\citation{SimonyanZ14a}
\citation{imagenet_cvpr09}
\citation{imagenet_cvpr09}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An illustration of the architecture of Alex Net, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network?s input is 150,528-dimensional, and the number of neurons in the network?s remaining layers is given by 253, 440-186, 624-64, 896-64, 896-43, 264-4096-4096-1000\cite  {NIPS2012_4824}.}}{9}}
\newlabel{fig:alexnet}{{5}{9}}
\citation{DBLP:journals/corr/HeZRS15}
\citation{SimonyanZ14a}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An illustration of the VGG network structure.}}{10}}
\newlabel{fig:vgg16}{{6}{10}}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An illustration of deep neural networks fails by using the traditional network structure}}{11}}
\newlabel{fig:resnet}{{7}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\cite  {DBLP:journals/corr/IoffeS15}}{11}}
\citation{DBLP:journals/corr/LinMBHPRDZ14}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:conf/iccv/Girshick15}
\citation{DBLP:conf/nips/RenHGS15}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{DBLP:journals/corr/RedmonF16}
\bibdata{jimmy_shen}
\bibstyle{ieeetr}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces (a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs. the number of training steps. Batch Normalization helps the network train faster and achieve higher accuracy\cite  {DBLP:journals/corr/IoffeS15}. (b, c) The evolution of input distributions to a typical sigmoid, over the course of training, shown as 15, 50, 85th percentiles. Batch Normalization makes the distribution more stable and reduces the internal covariate shift\cite  {DBLP:journals/corr/IoffeS15}.}}{12}}
\newlabel{fig:bp}{{8}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Object Detection}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Microsoft COCO: Common Objects in Context \cite  {DBLP:journals/corr/LinMBHPRDZ14} }{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Rich feature hierarchies for accurate object detection and semantic segmentation\cite  {DBLP:journals/corr/GirshickDDM13}}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Fast R-CNN\cite  {DBLP:conf/iccv/Girshick15}}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\cite  {DBLP:conf/nips/RenHGS15}}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}You Only Look Once: Unified, Real-Time Object Detection\cite  {DBLP:journals/corr/RedmonDGF15}}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}YOLO9000: Better, Faster, Stronger\cite  {DBLP:journals/corr/RedmonF16}}{12}}
