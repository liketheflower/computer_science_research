\relax 
\citation{ji2017}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{csaji2001}
\citation{csaji2001}
\citation{hornik1991}
\citation{hornik1991}
\citation{SimonyanZ14a}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Learning Theory}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Approximation with Artificial Neural Networks \cite  {csaji2001}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Approximation Capabilities of Multilayer Feedforward Networks \cite  {hornik1991} }{2}}
\citation{Rumelhart1986}
\citation{Rumelhart1986}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/IoffeS15}
\citation{doi:10.1162/neco.1989.1.4.541}
\citation{NIPS2012_4824}
\citation{SimonyanZ14a}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Learning representations by back-propagating errors\cite  {Rumelhart1986}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\cite  {DBLP:journals/corr/IoffeS15}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs. the number of training steps. Batch Normalization helps the network train faster and achieve higher accuracy\cite  {DBLP:journals/corr/IoffeS15}. (b, c) The evolution of input distributions to a typical sigmoid, over the course of training, shown as 15, 50, 85th percentiles. Batch Normalization makes the distribution more stable and reduces the internal covariate shift\cite  {DBLP:journals/corr/IoffeS15}.}}{3}}
\newlabel{fig:bp}{{1}{3}}
\citation{doi:10.1162/neco.1989.1.4.541}
\citation{imagenet_cvpr09}
\citation{imagenet_cvpr09}
\@writefile{toc}{\contentsline {section}{\numberline {3}Object Classification for 2D Images}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Backpropagation applied to handwritten zip code recognition\cite  {doi:10.1162/neco.1989.1.4.541}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Neural Network used in \cite  {doi:10.1162/neco.1989.1.4.541}.}}{4}}
\newlabel{fig:bpzip}{{2}{4}}
\citation{NIPS2012_4824}
\citation{DBLP:journals/corr/abs-1207-0580}
\citation{NIPS2012_4824}
\citation{NIPS2012_4824}
\citation{SimonyanZ14a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ImageNet: A Large-Scale Hierarchical Image Database\cite  {imagenet_cvpr09}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}ImageNet Classification with Deep Convolutional Neural Networks\cite  {NIPS2012_4824}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An illustration of the architecture of Alex Net, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network?s input is 150,528-dimensional, and the number of neurons in the network?s remaining layers is given by 253, 440-186, 624-64, 896-64, 896-43, 264-4096-4096-1000\cite  {NIPS2012_4824}.}}{5}}
\newlabel{fig:alexnet}{{3}{5}}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Very Deep Convolutional Networks for Large-Scale Image Recognition\cite  {SimonyanZ14a}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An illustration of the VGG network structure.}}{6}}
\newlabel{fig:vgg16}{{4}{6}}
\citation{DBLP:journals/corr/LinMBHPRDZ14}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:conf/iccv/Girshick15}
\citation{DBLP:conf/nips/RenHGS15}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{DBLP:journals/corr/RedmonF16}
\bibdata{jimmy_shen}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Deep Residual Learning for Image Recognition\cite  {DBLP:journals/corr/HeZRS15}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An illustration of deep neural networks.}}{7}}
\newlabel{fig:4}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Object Detection for 2D Images}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Microsoft COCO: Common Objects in Context \cite  {DBLP:journals/corr/LinMBHPRDZ14} }{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Rich feature hierarchies for accurate object detection and semantic segmentation\cite  {DBLP:journals/corr/GirshickDDM13}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Fast R-CNN\cite  {DBLP:conf/iccv/Girshick15}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\cite  {DBLP:conf/nips/RenHGS15}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}You Only Look Once: Unified, Real-Time Object Detection\cite  {DBLP:journals/corr/RedmonDGF15}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}YOLO9000: Better, Faster, Stronger\cite  {DBLP:journals/corr/RedmonF16}}{7}}
