\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
%\documentclass{article}
\usepackage{setspace, enumitem,titlesec}
\usepackage{calc}
			% Activate to display a given date or no date
\usepackage{mathtools}
\usepackage{mathrsfs }
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancybox}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cite}
%\usepackage{algpseudocode}
\begin{document}
%\renewcommand{\thepseudonum}{\roman{pseudonum}}
\renewcommand\labelenumi{(\theenumi)}
%vector
\renewcommand{\vec}[1]{\mathbf{#1}}
\title {Review of Week 1 }
%\author{jim.morris.shen@gmail.com}
%\author{The Graduate Center, City University of New York}
\author{Xiaoke(Jimmy) Shen}
%\author{The Graduate Center, City University of New York}
\maketitle
%\textbf{Due Mar 1st 11:59 pm. 10 points for each exercise and 20 points for the extra credit exercise }\\
\section{Introduction}

This paper review report is provided based on the requirement of the computer science research course provided by the Graduate Center of the City University of New York during the 2017 Fall Semester. The main objective of this course is helping the PHD candidates to prepare their second exam and identify their thesis topic as early as possible\cite{ji2017}. \\

\section{Deep Learning Theory}
\subsection{Approximation with Artificial Neural Networks \cite{csaji2001}}
In order to build the mathematical theory of the artificial neural networks, several papers are published in the 20 century. In this paper one main contribution is the universal approximation theorem with proof. The universal approximation theorem claims \cite{csaji2001}  that the standard multilayer feed-forward networks with a single hidden layer that contains finite number of hidden neurons, and with arbitrary activation function are universal approximators in $C(R^m)$.  The universal approximation theorem is one of the important theoretical support for the artificial neural networks. However, at that time as the huge size labeled data sets are not available, the updated algorithms haven't been invented and also the limited computation power, these ideas can not be verified.


\subsection{Approximation Capabilities of Multilayer Feedforward Networks \cite{hornik1991} }

The unique value of the Kurt Hornik (1991) \cite{hornik1991} paper is it showed that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. This is an important contribution which is the foundation for the current state of the art deep learning architecture such as VGG 16  \cite{SimonyanZ14a} and resnet \cite{DBLP:journals/corr/HeZRS15}




\subsection{Learning representations by back-propagating errors\cite{Rumelhart1986}}
The paper of 1986  of BP(Back Propagation) significantly contributed  to the Artificial Neural Networks \cite{Rumelhart1986}. It experimentally demonstrated the  usefulness of the internal representations of hidden layers. The Back Propagation algorithm is one of the most critical and fundamental algorithm used in the deep neural network. It gave an efficient way to update the weights of the Neural Network. However, as the limitation of the computation power and also some other unresolved problems at that time such as overfitting and big number of labeled data is not available, the importance of this algorithms is still not well known as it is shown recently.\\

               
\bibliography{jimmy_shen}
\bibliographystyle{ieeetr}
  
 \end{document}